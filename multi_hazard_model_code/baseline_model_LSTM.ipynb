{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# multi-hazard prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loading the NOAA dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data= pd.read_csv(\"data/processed_USA_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loading pressure level data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pressure_level_12= pd.read_csv(\"data/extracted csv_pressure_level/processed_pressure_level_12h.csv\")\n",
    "pressure_level_6= pd.read_csv(\"data/extracted csv_pressure_level/processed_pressure_level_6h.csv\")\n",
    "pressure_level_4= pd.read_csv(\"data/extracted csv_pressure_level/processed_pressure_level_4h.csv\")\n",
    "pressure_level_2= pd.read_csv(\"data/extracted csv_pressure_level/processed_pressure_level_2h.csv\")\n",
    "pressure_level_1= pd.read_csv(\"data/extracted csv_pressure_level/processed_pressure_level_1h.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_columns(df, suffix):\n",
    "    return df.rename(columns={col: f\"{col}_{suffix}\" for col in df.columns if col != \"file_id\"})\n",
    "\n",
    "# Rename columns with unique suffixes\n",
    "pressure_level_12 = rename_columns(pressure_level_12, \"12h\")\n",
    "pressure_level_6 = rename_columns(pressure_level_6, \"6h\")\n",
    "pressure_level_4 = rename_columns(pressure_level_4, \"4h\")\n",
    "pressure_level_2 = rename_columns(pressure_level_2, \"2h\")\n",
    "pressure_level_1 = rename_columns(pressure_level_1, \"1h\")\n",
    "\n",
    "# Merge datasets column-wise based on 'file_id'\n",
    "merged_data_pressure_level = pressure_level_12.merge(pressure_level_6, on=\"file_id\", how=\"inner\") \\\n",
    "                               .merge(pressure_level_4, on=\"file_id\", how=\"inner\") \\\n",
    "                               .merge(pressure_level_2, on=\"file_id\", how=\"inner\") \\\n",
    "                               .merge(pressure_level_1, on=\"file_id\", how=\"inner\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_data_pressure_level=pressure_level_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pressure_level_12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loading precipation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_precip=pd.read_csv(\"data/extracted_preciption_data_csv/era_5_precipitation_avg_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_precip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the specified columns\n",
    "avg_Precip_current = avg_precip[['id', 'avg_precip_current']]\n",
    "avg_precip_12 = avg_precip[['id', 'avg_precip_12h_early']]\n",
    "avg_precip_24 = avg_precip[['id', 'avg_precip_24h_early']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_Precip_current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NASA_features = data[['event_id', 'landslide_category', 'landslide_trigger','landslide_size','longitude','latitude']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NASA_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "merged_data_pressure_level = merged_data_pressure_level.rename(columns={\"file_id\": \"id\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NASA_features = NASA_features.rename(columns={'event_id': 'id'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merging data pressure level and NASA Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Merge the DataFrames on the 'ID' column\n",
    "merged_data = NASA_features.merge(avg_precip, on='id', how='inner').merge(merged_data_pressure_level, on='id', how='inner')\n",
    "\n",
    "# Display the first few rows of the merged DataFrame to verify\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for NULL or NaN values in the merged_data DataFrame\n",
    "null_summary = merged_data.isnull().sum()\n",
    "\n",
    "# Display columns with missing values\n",
    "print(\"Columns with NULL or NaN values:\")\n",
    "print(null_summary[null_summary > 0])\n",
    "\n",
    "# Alternatively, display the total count of missing values\n",
    "total_missing = merged_data.isnull().sum().sum()\n",
    "print(f\"\\nTotal number of missing values: {total_missing}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with any NaN values\n",
    "merged_data = merged_data.dropna()\n",
    "\n",
    "# Verify that there are no more NaN values\n",
    "print(\"Number of missing values after dropping NaN rows:\")\n",
    "print(merged_data.isnull().sum().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and print max, min, and mean of the 'avg_precip_current' column\n",
    "max_precip = merged_data['avg_precip_current'].max()\n",
    "min_precip = merged_data['avg_precip_current'].min()\n",
    "mean_precip = merged_data['avg_precip_current'].mean()\n",
    "\n",
    "print(f\"Maximum value of avg_precip_current: {max_precip}\")\n",
    "print(f\"Minimum value of avg_precip_current: {min_precip}\")\n",
    "print(f\"Mean value of avg_precip_current: {mean_precip}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace values less than 0 with 0 in specified columns\n",
    "columns_to_update = ['avg_precip_24h_early', 'avg_precip_12h_early', 'avg_precip_current']\n",
    "merged_data[columns_to_update] = merged_data[columns_to_update].applymap(lambda x: max(x, 0))\n",
    "\n",
    "# Print confirmation message\n",
    "print(\"All negative values in the specified columns have been set to 0.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count and print the number of values less than 0 in the 'avg_precip_current' column\n",
    "negative_values_count = (merged_data['avg_precip_current'] < 0).sum()\n",
    "print(f\"Number of values in 'avg_precip_current' less than 0: {negative_values_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing all record other than landslide and midslide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display unique values in the 'landslide_category' column\n",
    "unique_landslide_categories = merged_data['landslide_category'].unique()\n",
    "print(unique_landslide_categories)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows where 'landslide_category' is 'other'\n",
    "merged_data = merged_data[merged_data['landslide_category'] != 'other']\n",
    "\n",
    "# To verify the removal, you can check the unique categories again\n",
    "print(merged_data['landslide_category'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Data Preparation: Assuming 'merged_data' is already defined and preprocessed\n",
    "\n",
    "# Encode categorical variables\n",
    "label_encoders = {}\n",
    "for column in ['landslide_category', 'landslide_trigger', 'landslide_size']:\n",
    "    le = LabelEncoder()\n",
    "    merged_data[column] = le.fit_transform(merged_data[column])\n",
    "\n",
    "# Define features and targets\n",
    "feature_columns = [col for col in merged_data.columns if col not in ['id', 'landslide_category', 'landslide_trigger', 'landslide_size', 'avg_precip_current', 'longitude', 'latitude']]\n",
    "X = merged_data[feature_columns].values\n",
    "y_loc = merged_data[['latitude', 'longitude']].values\n",
    "\n",
    "# Scaling features\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "y_loc_scaled = scaler.fit_transform(y_loc)  # Scale location data\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_loc_scaled, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert to PyTorch tensors and reshape for LSTM\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32).view(-1, 1, X_train.shape[1]).to(device)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32).view(-1, 1, X_test.shape[1]).to(device)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).to(device)\n",
    "\n",
    "# Create TensorDataset and DataLoader for training and testing\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "predicting location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have loaded and scaled your data into X_tensor and y_loc_tensor\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_tensor, y_train_tensor, test_size=0.1, random_state=42)\n",
    "\n",
    "# Ensure the data has a sequence length dimension for LSTM\n",
    "# Add a sequence length of 1 for LSTM input\n",
    "# X_train = X_train.unsqueeze(1)  # Add a sequence dimension\n",
    "# X_val = X_val.unsqueeze(1)      # Add a sequence dimension\n",
    "\n",
    "# Convert tensors to the correct device\n",
    "X_train = X_train.to(device)\n",
    "y_train = y_train.to(device)\n",
    "X_val = X_val.to(device)\n",
    "y_val = y_val.to(device)\n",
    "\n",
    "# Create TensorDataset and DataLoader for the training and validation sets\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "val_dataset = TensorDataset(X_val, y_val)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "\n",
    "class AdvancedLSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers, dropout=0.5):\n",
    "        super(AdvancedLSTMModel, self).__init__()\n",
    "        # Initializing the LSTM with dropout and making it bidirectional\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, \n",
    "                            dropout=dropout, bidirectional=True)\n",
    "        # Adjusting the input feature size for the fully connected layer due to bidirectionality\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)  # hidden_dim * 2 because it's bidirectional\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state with zeros\n",
    "        h0 = torch.zeros(num_layers * 2, x.size(0), hidden_dim).to(device)  # *2 for bidirectionality\n",
    "        # Initialize cell state\n",
    "        c0 = torch.zeros(num_layers * 2, x.size(0), hidden_dim).to(device)\n",
    "        # Forward propagate LSTM\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        # Decode the hidden state of the last time step\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "\n",
    "# Initialize the model, criterion, and optimizer\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# model = LSTMModel(input_dim=X_train.shape[2], hidden_dim=64, output_dim=2, num_layers=2).to(device)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "input_dim = X_train.shape[2]  # example feature size\n",
    "hidden_dim = 64\n",
    "output_dim = 2  # example output dimension\n",
    "num_layers = 2\n",
    "dropout = 0.5\n",
    "\n",
    "# Model instantiation\n",
    "model = AdvancedLSTMModel(input_dim, hidden_dim, output_dim, num_layers, dropout).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Training and validation loop\n",
    "num_epochs = 200\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            total_val_loss += loss.item()\n",
    "    \n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "    val_losses.append(avg_val_loss)\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}')\n",
    "\n",
    "# Plotting the learning curves\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, r2_score,mean_squared_error\n",
    "\n",
    "# Validation loop\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    total_loss = 0\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    \n",
    "    for X_batch, y_batch in test_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(X_batch)\n",
    "        \n",
    "        # Reverse scaling for accurate performance measurement\n",
    "        outputs_np = outputs.cpu().numpy()  # Convert to numpy array on CPU\n",
    "        y_batch_np = y_batch.cpu().numpy()  # Convert to numpy array on CPU\n",
    "        \n",
    "        # Apply inverse transformation to scale back to original data space\n",
    "        outputs_rescaled = scaler.inverse_transform(outputs_np)\n",
    "        y_batch_rescaled = scaler.inverse_transform(y_batch_np)\n",
    "        \n",
    "        # Calculate loss on the original scale\n",
    "        loss = criterion(torch.tensor(outputs_rescaled), torch.tensor(y_batch_rescaled))\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        predictions.append(outputs_rescaled)\n",
    "        actuals.append(y_batch_rescaled)\n",
    "\n",
    "    # Calculate average loss\n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    print(f'Validation Loss: {avg_loss:.4f}')\n",
    "\n",
    "    # Flatten the lists for MAE and R2 calculations\n",
    "    predictions_flat = np.vstack(predictions)\n",
    "    actuals_flat = np.vstack(actuals)\n",
    "\n",
    "    # Compute MAE and R2\n",
    "    mae = mean_absolute_error(actuals_flat, predictions_flat)\n",
    "    r2 = r2_score(actuals_flat, predictions_flat)\n",
    "    mse = mean_squared_error(actuals_flat, predictions_flat)\n",
    "    print(f'Mean Absolute Error: {mae:.4f}')\n",
    "    print(f'R2 Score: {r2:.4f}')\n",
    "    print(f'MSE Score: {mse:.4f}')\n",
    "\n",
    "    # Display first 10 actual and predicted values\n",
    "    print(\"First 10 Actual and Predicted Values:\")\n",
    "    for i in range(10):\n",
    "        print(f\"Actual: {actuals_flat[i]}, Predicted: {predictions_flat[i]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting the precipation value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Define features and target\n",
    "feature_columns = [col for col in merged_data.columns if col not in ['id', 'landslide_category', 'landslide_trigger', 'landslide_size', 'avg_precip_current', 'longitude', 'latitude']]\n",
    "X = merged_data[feature_columns].values\n",
    "y_precip = merged_data['avg_precip_current'].values.reshape(-1, 1)  # Reshape for scaling\n",
    "\n",
    "# Scaling features and target\n",
    "scaler_X = MinMaxScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "X_scaled = scaler_X.fit_transform(X)\n",
    "y_precip_scaled = scaler_y.fit_transform(y_precip)  # Scale precipitation data\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_precip_scaled, test_size=0.2, random_state=42)\n",
    "\n",
    "# Now X_train, X_test, y_train, y_test are ready to be used for model training and testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming 'merged_data' is pre-loaded and contains the column 'avg_precip_current'\n",
    "\n",
    "# Define features and the target for prediction\n",
    "feature_columns = [col for col in merged_data.columns if col not in ['id', 'landslide_category', 'landslide_trigger', 'landslide_size', 'avg_precip_current', 'longitude', 'latitude']]\n",
    "X = merged_data[feature_columns].values\n",
    "y_precip = merged_data['avg_precip_current'].values.reshape(-1, 1)  # Ensure it's a 2D array for scaling\n",
    "\n",
    "# Scaling features and target\n",
    "scaler_X = MinMaxScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "X_scaled = scaler_X.fit_transform(X)\n",
    "y_precip_scaled = scaler_y.fit_transform(y_precip)\n",
    "\n",
    "# Split data into train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_scaled, y_precip_scaled, test_size=0.1, random_state=42)\n",
    "\n",
    "# Convert arrays to tensors and ensure correct shape for LSTM\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "X_val = torch.tensor(X_val, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "y_val = torch.tensor(y_val, dtype=torch.float32).to(device)\n",
    "\n",
    "# Create TensorDataset and DataLoader\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "val_dataset = TensorDataset(X_val, y_val)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the LSTM model adapted for single value output (precipitation prediction)\n",
    "class AdvancedLSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers, dropout=0.5):\n",
    "        super(AdvancedLSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)  # Adjust for bidirectionality\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(num_layers * 2, x.size(0), hidden_dim).to(device)\n",
    "        c0 = torch.zeros(num_layers * 2, x.size(0), hidden_dim).to(device)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# Model instantiation for precipitation prediction\n",
    "input_dim = X_train.shape[2]\n",
    "hidden_dim = 64\n",
    "output_dim = 1  # Predicting a single value\n",
    "num_layers = 2\n",
    "dropout = 0.5\n",
    "\n",
    "model = AdvancedLSTMModel(input_dim, hidden_dim, output_dim, num_layers, dropout).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training and validation loop\n",
    "num_epochs = 200\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            total_val_loss += loss.item()\n",
    "    \n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "    val_losses.append(avg_val_loss)\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}')\n",
    "\n",
    "# Plotting the learning curves\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error\n",
    "\n",
    "# Assume 'scaler_y' is the MinMaxScaler instance used to scale the 'avg_precip_current' during preprocessing\n",
    "\n",
    "# Validation loop\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    total_loss = 0\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    \n",
    "    for X_batch, y_batch in val_loader:  # Ensure you use the correct DataLoader here, i.e., val_loader\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(X_batch)\n",
    "        \n",
    "        # Convert tensors to numpy arrays for inverse scaling\n",
    "        outputs_np = outputs.cpu().numpy()  # Convert to numpy array on CPU\n",
    "        y_batch_np = y_batch.cpu().numpy()  # Convert to numpy array on CPU\n",
    "        \n",
    "        # Apply inverse transformation to scale back to original data space\n",
    "        outputs_rescaled = scaler_y.inverse_transform(outputs_np)\n",
    "        y_batch_rescaled = scaler_y.inverse_transform(y_batch_np)\n",
    "        \n",
    "        # Collect predictions and actual values for metric calculations\n",
    "        predictions.append(outputs_rescaled)\n",
    "        actuals.append(y_batch_rescaled)\n",
    "\n",
    "    # Flatten the lists for metrics calculation\n",
    "    predictions_flat = np.vstack(predictions)\n",
    "    actuals_flat = np.vstack(actuals)\n",
    "\n",
    "    # Compute metrics\n",
    "    mae = mean_absolute_error(actuals_flat, predictions_flat)\n",
    "    r2 = r2_score(actuals_flat, predictions_flat)\n",
    "    mse = mean_squared_error(actuals_flat, predictions_flat)\n",
    "\n",
    "    # Print performance metrics\n",
    "    print(f'Mean Absolute Error (MAE): {mae:.4f}')\n",
    "    print(f'R2 Score: {r2:.4f}')\n",
    "    print(f'Mean Squared Error (MSE): {mse:.4f}')\n",
    "\n",
    "    # Display first 10 actual and predicted values\n",
    "    print(\"First 10 Actual and Predicted Values:\")\n",
    "    for i in range(10):\n",
    "        print(f\"Actual: {actuals_flat[i][0]:.4f}, Predicted: {predictions_flat[i][0]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "predicting landslide category and trigger point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "\n",
    "# Device configuration for PyTorch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Assuming 'merged_data' is already defined and preprocessed\n",
    "\n",
    "# Encode categorical variables\n",
    "label_encoders = {}\n",
    "categorical_columns = ['landslide_category', 'landslide_trigger', 'landslide_size']\n",
    "for column in categorical_columns:\n",
    "    if column in merged_data.columns:\n",
    "        le = LabelEncoder()\n",
    "        merged_data[column] = le.fit_transform(merged_data[column])\n",
    "        label_encoders[column] = le  # Store label encoder for potential inverse transformation\n",
    "\n",
    "# Define features and new targets\n",
    "feature_columns = [col for col in merged_data.columns if col not in ['id', 'landslide_category', 'landslide_trigger', 'landslide_size', 'avg_precip_current', 'longitude', 'latitude']]\n",
    "X = merged_data[feature_columns].values\n",
    "y_category = merged_data['landslide_category'].values\n",
    "y_trigger = merged_data['landslide_trigger'].values\n",
    "\n",
    "# Scaling features\n",
    "scaler_X = MinMaxScaler()\n",
    "X_scaled = scaler_X.fit_transform(X)\n",
    "\n",
    "# Split data into train and test sets for both targets\n",
    "X_train, X_test, y_cat_train, y_cat_test = train_test_split(X_scaled, y_category, test_size=0.2, random_state=42)\n",
    "_, _, y_trig_train, y_trig_test = train_test_split(X_scaled, y_trigger, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_cat_train = torch.tensor(y_cat_train, dtype=torch.long)\n",
    "y_cat_test = torch.tensor(y_cat_test, dtype=torch.long)\n",
    "y_trig_train = torch.tensor(y_trig_train, dtype=torch.long)\n",
    "y_trig_test = torch.tensor(y_trig_test, dtype=torch.long)\n",
    "\n",
    "# Create TensorDataset and DataLoader for training\n",
    "train_dataset_cat = TensorDataset(X_train, y_cat_train)\n",
    "train_loader_cat = DataLoader(train_dataset_cat, batch_size=64, shuffle=True)\n",
    "train_dataset_trig = TensorDataset(X_train, y_trig_train)\n",
    "train_loader_trig = DataLoader(train_dataset_trig, batch_size=64, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define an example neural network model for classification\n",
    "class ClassificationModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(ClassificationModel, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.layer1(x))\n",
    "        x = self.softmax(self.layer2(x))\n",
    "        return x\n",
    "\n",
    "# Number of unique categories and triggers\n",
    "num_categories = len(np.unique(y_category))\n",
    "num_triggers = len(np.unique(y_trigger))\n",
    "\n",
    "# Initialize the model for landslide category prediction\n",
    "model_cat = ClassificationModel(input_dim=len(feature_columns), hidden_dim=100, output_dim=num_categories).to(device)\n",
    "model_trig = ClassificationModel(input_dim=len(feature_columns), hidden_dim=100, output_dim=num_triggers).to(device)\n",
    "\n",
    "# Setup the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_cat = torch.optim.Adam(model_cat.parameters(), lr=0.001)\n",
    "optimizer_trig = torch.optim.Adam(model_trig.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Setup data loaders for testing\n",
    "test_dataset_cat = TensorDataset(torch.tensor(X_test, dtype=torch.float32), torch.tensor(y_cat_test, dtype=torch.long))\n",
    "test_loader_cat = DataLoader(test_dataset_cat, batch_size=64, shuffle=False)\n",
    "\n",
    "test_dataset_trig = TensorDataset(torch.tensor(X_test, dtype=torch.float32), torch.tensor(y_trig_test, dtype=torch.long))\n",
    "test_loader_trig = DataLoader(test_dataset_trig, batch_size=64, shuffle=False)\n",
    "\n",
    "# Training function\n",
    "def train_model(model, train_loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "# Testing function\n",
    "def test_model(model, test_loader):\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            outputs = model(X_batch)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            y_true.extend(y_batch.cpu().numpy())\n",
    "            y_pred.extend(predicted.cpu().numpy())\n",
    "    return y_true, y_pred\n",
    "\n",
    "# Run training and testing\n",
    "num_epochs = 200\n",
    "for epoch in range(num_epochs):\n",
    "    loss_cat = train_model(model_cat, train_loader_cat, optimizer_cat, criterion)\n",
    "    loss_trig = train_model(model_trig, train_loader_trig, optimizer_trig, criterion)\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss Cat: {loss_cat:.4f}, Loss Trig: {loss_trig:.4f}')\n",
    "\n",
    "# Testing and classification report for category\n",
    "y_true_cat, y_pred_cat = test_model(model_cat, test_loader_cat)\n",
    "print(\"Classification Report for Landslide Category:\")\n",
    "print(classification_report(y_true_cat, y_pred_cat))\n",
    "\n",
    "# Testing and classification report for trigger\n",
    "y_true_trig, y_pred_trig = test_model(model_trig, test_loader_trig)\n",
    "print(\"Classification Report for Landslide Trigger:\")\n",
    "print(classification_report(y_true_trig, y_pred_trig))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorchgpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
